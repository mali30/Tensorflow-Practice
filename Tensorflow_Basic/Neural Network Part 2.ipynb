{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "1.15.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of by defining number of labels we will have aka # of features\n",
    "number_of_labels = 10\n",
    "\n",
    "# define the number of neurons we will use. 1 layer 3 neurons\n",
    "number_of_neurons = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our placeholder\n",
    "# z = Wx + b\n",
    "# y = Mx + b\n",
    "\n",
    "# number of samples by the number of features\n",
    "# we dont know our sample size which is why we put None\n",
    "# we know our number of features though which is 10\n",
    "\n",
    "x = tf.placeholder(tf.float32,(None,number_of_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight = tf.Variable(tf.random_normal([number_of_labels, number_of_neurons]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias = tf.Variable(tf.ones(number_of_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xW = tf.matmul(x, Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.add(xW,Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # result of each neuron computed\n",
    "    layer_out = sess.run(a, feed_dict={x:np.random.random([1,number_of_labels])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.892269  0.8575778 0.9064101]]\n"
     ]
    }
   ],
   "source": [
    "# we only ran it once so its not really a nn. We need some sort of cost function\n",
    "# so that we can adjust our Weight and Bias.\n",
    "print(layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE REGRESSION EXAMPLE\n",
    "# PERFORMING A LINEAR FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some noise\n",
    "x_data = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23408387,  1.16733385,  2.84323153,  2.18419812,  3.53073642,\n",
       "        4.67157735,  5.55673775,  6.81475607,  8.2123665 , 11.25344575])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81119736,  1.27087491,  1.62025442,  3.93673148,  5.91030852,\n",
       "        6.56042587,  6.89352983,  7.18621313,  8.30194083, 11.15389827])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11bcae908>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_data,y_label, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want y = mx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34835795, 0.02099754])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we take two random variables and in the end our nn will take care of \n",
    "# the work for us by using the cost and optimized fuction \n",
    "# here m is the slope\n",
    "# b is the intercept\n",
    "m = tf.Variable(0.73)\n",
    "b = tf.Variable(0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = 0\n",
    "\n",
    "for x,y in zip(x_data, y_label):\n",
    "    \n",
    "    # y_hat here means our predicted value. it will probably be way off \n",
    "    # since we are using random values. thats where are cost function comes\n",
    "    # into play\n",
    "    y_hat = m*x + b\n",
    "    \n",
    "    # here we take our true y value and subtract it from our predicted y value\n",
    "    # then we square it to punish higher error \n",
    "    cost_function += (y - y_label) ** 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use an optizmizer to minimize the error\n",
    "# learning rate defines how fast we are going to descend down\n",
    "#  too large is not good too small is gonna take forever\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'base_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-24c9a2fa335e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now we tell it what it is trying to minimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    364\u001b[0m                        \u001b[0;34m\"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                        gate_gradients)\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_assert_valid_dtypes\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0mvalid_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'base_dtype'"
     ]
    }
   ],
   "source": [
    "# now we tell it what it is trying to minimize\n",
    "train = optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bd52b1b37806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfinal_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_intercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(start)\n",
    "    \n",
    "    training_steps = 1 \n",
    "    \n",
    "    for i in range(training_steps):\n",
    "        \n",
    "        sess.run(train)\n",
    "    \n",
    "        final_slope, final_intercept = sess.run([m,b])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_slope' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bd4c1e42346c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Now y = mx + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_slope\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfinal_intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_slope' is not defined"
     ]
    }
   ],
   "source": [
    "x_test = np.linspace(-1,11,10)\n",
    "\n",
    "# Now y = mx + b\n",
    "\n",
    "final_output = final_slope * x_test + final_intercept\n",
    "\n",
    "plt.plot(x_test, final_output)\n",
    "plt.plot(x_data,y_label, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
