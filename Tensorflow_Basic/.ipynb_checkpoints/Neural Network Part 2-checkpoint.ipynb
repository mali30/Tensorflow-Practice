{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "print(np.__version__)\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of by defining number of labels we will have aka # of features\n",
    "number_of_labels = 10\n",
    "\n",
    "# define the number of neurons we will use. 1 layer 3 neurons\n",
    "number_of_neurons = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our placeholder\n",
    "# z = Wx + b\n",
    "# y = Mx + b\n",
    "\n",
    "# number of samples by the number of features\n",
    "# we dont know our sample size which is why we put None\n",
    "# we know our number of features though which is 10\n",
    "\n",
    "x = tf.placeholder(tf.float32,(None,number_of_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight = tf.Variable(tf.random_normal([number_of_labels, number_of_neurons]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias = tf.Variable(tf.ones(number_of_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xW = tf.matmul(x, Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.add(xW,Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # result of each neuron computed\n",
    "    layer_out = sess.run(a, feed_dict={x:np.random.random([1,number_of_labels])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.943276   0.96317124 0.9742242 ]]\n"
     ]
    }
   ],
   "source": [
    "# we only ran it once so its not really a nn. We need some sort of cost function\n",
    "# so that we can adjust our Weight and Bias.\n",
    "print(layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE REGRESSION EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some noise\n",
    "x_data = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85637022, 0.11070813, 0.96500754, 2.64529598, 4.43580517,\n",
       "       4.41489898, 5.42008952, 8.49972799, 8.26827872, 9.6485817 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.76987312,  1.11274695,  3.19280263,  2.59889999,  5.70579149,\n",
       "        4.42831085,  7.3685103 ,  8.76343839,  8.10711932, 11.22950085])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x123062940>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_data,y_label, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want y = mx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43665094, 0.62529635])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we take two random variables and in the end our nn will take care of \n",
    "# the work for us by using the cost and optimized fuction \n",
    "m = tf.Variable(0.73)\n",
    "b = tf.Variable(0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = 0\n",
    "\n",
    "for x,y in zip(x_data, y_label):\n",
    "    \n",
    "    # y_hat here means our predicted value. it will probably be way off \n",
    "    # since we are using random values. thats where are cost function comes\n",
    "    # into play\n",
    "    y_hat = m*x + b\n",
    "    \n",
    "    # here we take our true y value and subtract it from our predicted y value\n",
    "    # then we square it to punish higher error \n",
    "    cost_function += (y - y_label) ** 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use an optizmizer to minimize the error\n",
    "# learning rate defines how fast we are going to descend down\n",
    "#  too large is not good too small is gonna take forever\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'base_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fc6ecd3a15fa>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    364\u001b[0m                        \u001b[0;34m\"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                        gate_gradients)\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_assert_valid_dtypes\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0mvalid_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         raise ValueError(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'base_dtype'"
     ]
    }
   ],
   "source": [
    "# now we tell it what it is trying to minimize\n",
    "train = optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
