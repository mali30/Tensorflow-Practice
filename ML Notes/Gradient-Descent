Gradient Descent is an optimization algorithm for finding the minimum of a function
* Useful since we want to minimize the cost function

To find the local minimum, we take steps proportional to the negative of the gradient

Gradient- derivative of the function at a point
We keep taking step by step on the gradient and then get the minimum cost

Using the gradient descent, we can figure out the best parameters for minimizing our cost for example, finding the best values for the weights for our neuron inputs

How can we optimize our parameters or weights across our network? This is where BackProgration comes in.

BackProgration is used to calculate the error contribution of each neuron after a batch of data is processed
* Relies heavliy on the chain rule and goes back through the network.
* It calculates the error at the output then distributes back through the network layers
* It requires a known desired output for each input value (supervised learning)


